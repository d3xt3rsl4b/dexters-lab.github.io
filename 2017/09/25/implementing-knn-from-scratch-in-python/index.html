<!doctype html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>Implementing K-NearestNeighbour algorithm from scratch in python - Tainted Bits</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Tainted Bits"><meta name="msapplication-TileImage" content="/images/logo-header.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Tainted Bits"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="K-Nearest Neighbour is the simplest of machine learning algorithms which can be very effective in some cases. The objective of the post it to implement it from scratch in python, you need to know a fa"><meta property="og:type" content="article"><meta property="og:title" content="Implementing K-NearestNeighbour algorithm from scratch in python"><meta property="og:url" content="https://www.taintedbits.com/2017/09/25/implementing-knn-from-scratch-in-python/"><meta property="og:site_name" content="Tainted Bits"><meta property="og:description" content="K-Nearest Neighbour is the simplest of machine learning algorithms which can be very effective in some cases. The objective of the post it to implement it from scratch in python, you need to know a fa"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.taintedbits.com/2017/09/25/implementing-knn-from-scratch-in-python/knn_classifier_performance.png"><meta property="article:published_time" content="2017-09-25T09:00:44.000Z"><meta property="article:modified_time" content="2021-03-11T10:27:20.877Z"><meta property="article:author" content="D3xt3r"><meta property="article:tag" content="Classification"><meta property="article:tag" content="Regression"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2017/09/25/implementing-knn-from-scratch-in-python/knn_classifier_performance.png"><meta property="twitter:creator" content="@0xd3xt3r"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.taintedbits.com/2017/09/25/implementing-knn-from-scratch-in-python/"},"headline":"Tainted Bits","image":["https://www.taintedbits.com/2017/09/25/implementing-knn-from-scratch-in-python/knn_classifier_performance.png"],"datePublished":"2017-09-25T09:00:44.000Z","dateModified":"2021-03-11T10:27:20.877Z","author":{"@type":"Person","name":"D3xt3r"},"description":"K-Nearest Neighbour is the simplest of machine learning algorithms which can be very effective in some cases. The objective of the post it to implement it from scratch in python, you need to know a fa"}</script><link rel="alternate" href="/atom.xml" title="Tainted Bits" type="application/atom+xml"><link rel="icon" href="/images/logo-header.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/railscasts.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-128181439-1" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-128181439-1")</script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo-header.png" alt="Tainted Bits" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/publications">Publication</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/0xd3xt3r"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2017-09-25T09:00:44.000Z" title="9/25/2017, 2:30:44 PM">2017-09-25</time></span><span class="level-item">Updated&nbsp;<time datetime="2021-03-11T10:27:20.877Z" title="3/11/2021, 3:57:20 PM">2021-03-11</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">18 minutes read (About 2774 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Implementing K-NearestNeighbour algorithm from scratch in python</h1><div class="content"><p>K-Nearest Neighbour is the simplest of machine learning algorithms which can be very effective in some cases. The objective of the post it to implement it from scratch in python, you need to know a fair bit of python for and a little bit of numpy for the faster version of the algorithm. Once we have implemented the algorithm we will also see how to improve the performance of the algorithm. As there is no single invincible algorithm, we will look into advantage/disadvantage of the algorithm, this will help us to decide on when to use the algorithm. Alright, then let’s get straight into it.</p><span id="more"></span><h3 id="What-is-K-Nearest-Neighbour-Algorithm-KNN"><a href="#What-is-K-Nearest-Neighbour-Algorithm-KNN" class="headerlink" title="What is K-Nearest Neighbour Algorithm (KNN)?"></a>What is K-Nearest Neighbour Algorithm (KNN)?</h3><p>KNN is a supervised learning algorithm it means we need some label training data to use the algorithm. KNN is also called non-parametric algorithm as it makes no explicit assumption about the form of data, unlike any other parametric machine learning algorithm it does not have to estimate any parameter like the linear regression for it to work. It is also a lazy algorithm as the algorithm doesn’t run until you have to make the prediction.</p><p>In a typical machine learning algorithm, we are trying to predict the certain outcome base on data points (ML folks call it features). For example, if we are trying to predict the type of Iris flower based on petal height, width and sepal height, width. So we have 4 feature here and we are trying to classify the type of iris flower(assuming its a classification problem). If we plot feature on the graph and when we have to make the prediction for a data point, we try to find the nearest K data point and take the average value of the outcomes of those nearest data points.We can use KNN for both regression and classification problem. They both use the nearest data point in a different way to so their respective problem. If it’s the classification problem then we take the class that occurs the most in the K data points. And if it’s the regression problem then we take the average of the values of the K data points. There are also other variants of the KNN which is called weighted KNN which we take weight average of the K data points for both classification and regression problem. K in KNN is the positive natural number i.e. K &gt; 1, its the number of neighboring data points to consider when deciding the result. It’s a critical component of the algorithm which we will see how to choose.</p><p>When I say nearest data point, how do I calculate the distance between points, it’s an interesting question. Well, it’s very simple to calculate the distance between two points in one dimension like the distance between 5 and 12 by simple subtraction it’s 7 but in higher dimension we use Euclidean distance to measure the distance between two points, there are other approaches as well like man-hantten distance, cosine distance, and hamming distance. The goal here is to find similarity in data points, finding similarity in higher dimension is a challenge in itself as we progress with the post we will see why.</p><h3 id="Calculating-distance-between-data-points"><a href="#Calculating-distance-between-data-points" class="headerlink" title="Calculating distance between data points"></a>Calculating distance between data points</h3><p>We first need to calculate the distance between two data point for any given dimension, once we have that function we will use of calculating the distance between test data points to every other data point in the training set. As we are using Euclidean distance, it can be defined as the square root of the square of the difference between two points. Below is the mathematical formula for calculating the distance between two points.</p><p>$$dist = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + … + (x_n-y_n)^2}$$<br>$$dist = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}$$</p><p>Python implementation for the above equation is as below</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean_distance</span>(<span class="params">data_point1, data_point2</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; returns the distance between data_point1 and data_point2&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_point1) != <span class="built_in">len</span>(data_point2) :</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;feature length not matching&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data_point1)):</span><br><span class="line">            distance += <span class="built_in">pow</span>((data_point1[x] - data_point2[x]), <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> math.sqrt(distance)</span><br></pre></td></tr></table></figure><p>Now that we can calculate distance between two data point we are now interested in finding K nearest neighbouring data points, for that we first find the distance between one point and every other point and sort the distance in ascending order and take the first K points, those will be the data point we are interested to be used to calculate our next result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_neighbors</span>(<span class="params">train_set_data_points, test_feature_data_point, k</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; returns the index the index of K nearest neighbour&#x27;&#x27;&#x27;</span></span><br><span class="line">       distances = []</span><br><span class="line">       length = <span class="built_in">len</span>(test_feature_data_point)-<span class="number">1</span></span><br><span class="line">       <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_set_data_points)):</span><br><span class="line">           dist = euclidean_distance(test_feature_data_point, train_set_data_points[index])</span><br><span class="line">           distances.append((train_set_data_points[index], dist, index))</span><br><span class="line">       distances.sort(key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">       neighbors = []</span><br><span class="line">       <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">           neighbors.append(distances[index][<span class="number">2</span>])</span><br><span class="line">       <span class="keyword">return</span> neighbors</span><br></pre></td></tr></table></figure><p>let try to test the above function with some data points and see if it’s doing its job right.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>euclidean_distance([<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1.4142135623730951</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>get_neighbors([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]],[<span class="number">2</span>,<span class="number">3</span>],<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="number">1</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>Since these are the common methods that can be used with both version of KNN classification and regression it would be nice to put in <strong>KnnBase</strong> class then this class can be inherited by <strong>KnnRegression</strong> class which implements KNN regression algorithm and <strong>KnnClassifier</strong> which implements KNN classification algorithm. API will be a sklearn style which has parameter initialization in class constructor and class has fit and predict method. If you are following along you can just copy-paste the code if you are feeling lazy to type it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KnnBase</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k, weights=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.weights = weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">euclidean_distance</span>(<span class="params">self, data_point1, data_point2</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(data_point1) != <span class="built_in">len</span>(data_point2) :</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;feature length not matching&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            distance = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data_point1)):</span><br><span class="line">                distance += <span class="built_in">pow</span>((data_point1[x] - data_point2[x]), <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> math.sqrt(distance)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, train_feature, train_label</span>):</span></span><br><span class="line">        self.train_feature = train_feature</span><br><span class="line">        self.train_label = train_label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_neighbors</span>(<span class="params">self, train_set_data_points, test_feature_data_point, k</span>):</span></span><br><span class="line">        distances = []</span><br><span class="line">        length = <span class="built_in">len</span>(test_feature_data_point)-<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_set_data_points)):</span><br><span class="line">            dist = self.euclidean_distance(test_feature_data_point, train_set_data_points[index])</span><br><span class="line">            distances.append((train_set_data_points[index], dist, index))</span><br><span class="line">        distances.sort(key=operator.itemgetter(<span class="number">1</span>))</span><br><span class="line">        neighbors = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            neighbors.append(distances[index][<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> neighbors</span><br></pre></td></tr></table></figure><h3 id="Implementing-KNN-classifier"><a href="#Implementing-KNN-classifier" class="headerlink" title="Implementing KNN classifier"></a>Implementing KNN classifier</h3><p>Predicting the outcome of the KNN classification algorithm is different than that of regression. For classification problem, we take votes of K closest neighbor and the class that has highest votes wins.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KnnClassifier</span>(<span class="params">KnnBase</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, test_feature_data_point</span>):</span></span><br><span class="line">        <span class="comment"># get the index of all nearest neighbouring data points</span></span><br><span class="line">        nearest_data_point_index = self.get_neighbors(self.train_feature, test_feature_data_point, self.k)</span><br><span class="line">        vote_counter = &#123;&#125;</span><br><span class="line">        <span class="comment"># to count votes for each class initialise all class with zero votes</span></span><br><span class="line">        print(<span class="string">&#x27;Nearest Data point index &#x27;</span>, nearest_data_point_index)</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> <span class="built_in">set</span>(self.train_label):</span><br><span class="line">            vote_counter[label] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># add count to class that are present in the nearest neighbors data points</span></span><br><span class="line">        <span class="keyword">for</span> class_index <span class="keyword">in</span> nearest_data_point_index:</span><br><span class="line">            closest_lable = self.train_label[class_index]</span><br><span class="line">            vote_counter[closest_lable] += <span class="number">1</span></span><br><span class="line">        print(<span class="string">&#x27;Nearest data point count&#x27;</span>, vote_counter)</span><br><span class="line">        <span class="comment"># return the class that has most votes</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(vote_counter.items(), key = operator.itemgetter(<span class="number">1</span>))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="Implementing-KNN-Regression"><a href="#Implementing-KNN-Regression" class="headerlink" title="Implementing KNN Regression"></a>Implementing KNN Regression</h3><p>For regression setting, we just take the average of the output of the K closet training data points.</p><p>$$output = {1 \over K}{\sum_{i=1}^n (y_i)}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KnnRegression</span>(<span class="params">KnnBase</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, test_feature_data_point</span>):</span></span><br><span class="line">        nearest_data_point_index = self.get_neighbors(self.train_feature, test_feature_data_point, self.k)</span><br><span class="line">        total_val = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># calculate the sum of all the label values</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> nearest_data_point_index:</span><br><span class="line">            total_val += self.train_label[index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> total_val/self.k</span><br></pre></td></tr></table></figure><h3 id="Optimized-the-computation-KNN-vectorized-version"><a href="#Optimized-the-computation-KNN-vectorized-version" class="headerlink" title="Optimized the computation (KNN vectorized version)"></a>Optimized the computation (KNN vectorized version)</h3><p>Above implementation which we saw using python array is very memory hungry and very slow which will be obvious when you run this algorithm on large dataset. To mitigate that we can vectorize the array operations using numpy library which can do vector computation much faster.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_neighbors_v</span>(<span class="params">train_set, test_set, k</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; return k closet neighbour of test_set in training set&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># calculate euclidean distance</span></span><br><span class="line">    euc_distance = np.sqrt(np.<span class="built_in">sum</span>((train_set - test_set)**<span class="number">2</span> , axis=<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># return the index of nearest neighbour</span></span><br><span class="line">    <span class="keyword">return</span> np.argsort(euc_distance)[<span class="number">0</span>:k]</span><br></pre></td></tr></table></figure><p>to compare the performance of both the function we will use ipython’s <strong>%timeit</strong> magic function and run the function for 10000 time and compare the results. So fire a ipython console and type the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">In [<span class="number">2</span>]: iris = datasets.load_iris()</span><br><span class="line">In [<span class="number">3</span>]: X = iris.data</span><br><span class="line">In [<span class="number">4</span>]: %timeit -n <span class="number">10000</span> get_neighbors_v(X,X[<span class="number">0</span>],<span class="number">10</span>)</span><br><span class="line"><span class="number">17.3</span> µs ± <span class="number">232</span> ns per loop (mean ± std. dev. of <span class="number">7</span> runs, <span class="number">10000</span> loops each)</span><br><span class="line">In [<span class="number">5</span>]: %timeit -n <span class="number">10000</span> get_neighbors(X, X[<span class="number">0</span>],<span class="number">10</span>)</span><br><span class="line"><span class="number">674</span> µs ± <span class="number">70.3</span> µs per loop (mean ± std. dev. of <span class="number">7</span> runs, <span class="number">10000</span> loops each)</span><br></pre></td></tr></table></figure><p>as you can see the result are 40x faster. Crude implementation using python’s native array takes 674µs to execute while vectorized version takes 17.3µs to execute 10000 function calls. You can replace the get_neighbour function code with the vectorized version in the KnnBase class to take advantage of vectorized version of the code.</p><h3 id="Evaluate-the-performance-of-the-KNN"><a href="#Evaluate-the-performance-of-the-KNN" class="headerlink" title="Evaluate the performance of the KNN"></a>Evaluate the performance of the KNN</h3><p>Evaluation the performance of the model is a topic in itself and entire books are written on the topic so I will try to keep it simple. Algorithm evaluation is how good our model performers based on certain metrics which will discuss, evaluation techniques are different for classification and regression model.</p><p>For regression model there are various metrics that can be used, one such metrics is Root mean square error (RMSE) which is the measure of the standard deviation from the ground truth which is a scale dependent. If we want something scale independent then we can use Mean absolute percentage error(MAPE) it expresses the accuracy in percentage. A small RMSE/MAPE means a good model and large means, bad model. Below is the code for calculating for both RMSE and MAPE metrics.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_rmse</span>(<span class="params">y, y_pred</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Root Mean Square Error</span></span><br><span class="line"><span class="string">    https://en.wikipedia.org/wiki/Root-mean-square_deviation</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    mse = np.mean((y - y_pred)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sqrt(mse)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_mape</span>(<span class="params">y, y_pred</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Mean Absolute Percent Error</span></span><br><span class="line"><span class="string">    https://en.wikipedia.org/wiki/Mean_absolute_percentage_error</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    perc_err = (<span class="number">100</span>*(y - y_pred))/y</span><br><span class="line">    <span class="keyword">return</span> np.mean(<span class="built_in">abs</span>(perc_err))</span><br></pre></td></tr></table></figure><p>For KNN classification settings usually we use metrics like precision, recall, and f1 score but for sake of simplicity, we will measure the percentage of data that is correctly classified so if only 10% of the data is correctly classified we say that the model is 10% accuracy. A larger value is better. Below is the implementation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span>(<span class="params">y, y_pred</span>):</span></span><br><span class="line">    cnt = (y == y_pred).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(cnt/<span class="built_in">len</span>(y), <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="Data-pre-processing"><a href="#Data-pre-processing" class="headerlink" title="Data pre-processing"></a>Data pre-processing</h3><p>One problem one might run into using KNN is that the feature vector might be on different scale, for example, you have a features like height, weight, and daily expense, height is on inch scale whose value ranging from 2 to 100 , weight is on kg scale whose value might range in 10 to 200 and daily expense is on dollar that might range from 0 to million who knows.<br>A feature that has large scale can influence the decision of the algorithm even though the feature itself in is not actually contributing anything to the output. To tackle this problem we can bring each feature on same scale by either scale the feature in the 0 to 1 range using <strong>Normalization</strong> or use <strong>Standardization</strong> method that transforms the feature such that it has mean 0 and standard deviation 1 but to using standardization the assumption is that the feature follows normal distribution so use standardization only if the feature satisfies the assumption. For feature normalization, we will use sklearn <strong>MinMaxScale</strong> class.</p><h3 id="Choosing-optimal-K-for-KNN"><a href="#Choosing-optimal-K-for-KNN" class="headerlink" title="Choosing optimal K for KNN"></a>Choosing optimal K for KNN</h3><p>To get high performance for the model it is important to choosing the optimal value of K. So K is the tuning parameter for KNN algorithm. Since we know how to assess the performance from the above section we can plot the value of K vs the evaluation metrics and choose the K which gives maximum performance. Below is the code to do so.<br>For this example we will use iris data set which is a classification problem to try the concept :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> knn <span class="keyword">import</span> KnnClassifier, get_accuracy</span><br><span class="line"><span class="comment"># load the iris data set</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">knn_iris_acc = []</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">scale = MinMaxScaler()</span><br><span class="line">X = scale.fit_transform(X)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(iris.data)):</span><br><span class="line">    clf = KnnClassifier(k)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    iris_pred = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">        pred = clf.predict(x)</span><br><span class="line">        iris_pred.append(pred)</span><br><span class="line">    iris_target_pred = np.array(iris_pred)</span><br><span class="line">    knn_iris_acc.append(get_accuracy(iris_target_pred, iris.target))</span><br><span class="line"></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">2</span>,<span class="built_in">len</span>(iris.data)), knn_iris_acc)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of neighbours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/2017/09/25/implementing-knn-from-scratch-in-python/knn_classifier_performance.png" title="Accuracy v&#x2F;s Number of neighbour plot"><p>If you see the plot performance varies for different values of K, for K=2 we only consider nearest 2 data point and for <code>K=len(iris.data)</code>(all the data point) which is the average prediction for all data point performance varies and max value is 98% for K=13, performance decline as K increases and there is sharp decline after K=100 and 140 and its lowest for <code>K=len(iris.data)</code> which is 55%. We can use this same technique for regression class of problem by plotting.</p><h3 id="Bias-vs-Variance-trade-off"><a href="#Bias-vs-Variance-trade-off" class="headerlink" title="Bias vs Variance trade-off"></a>Bias vs Variance trade-off</h3><p>As we saw the of the value of K has a drastic effect on the performance of the algorithm. For K=1 the decision boundary is highly flexible since the value of the nearest data point is directly applied so for this setting algorithm has low bias and high variance but as the value of K increases decision boundary becomes more and more inflexible i.e close to linear and this is close to high bias and low variance. Ideally, we want to have both bias and variance to be low but it can’t be so we are looking for a trade-off that in our iris data set was achieved for K=13 where we get optimal performance.</p><h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h3><ol><li>If you are using linear regression or other statistical learning methods than those algorithms have some assumption on the data set like they should be normally distributed or error should be uncorrelated etc. but for KNN there is no such assumption made about the data.</li><li>For KNN there are is only one parameter to tune i.e K, unlike regression where if you have N feature then there might be at least N parameters to tune.</li></ol><h3 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h3><ol><li><p>Machine learning algorithms that work in low dimension might fail to produce the same result for high-dimensional setting especially similarity base algorithm like KNN. A good intuition behind this is very well explained by Pedro Domingos in one of his research paper which is as follows :</p><blockquote><p>our intuitions, which come from a three-dimensional world, often do not apply in high-dimensional<br>ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in<br>a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor.</p><footer><strong>Pedro Domingos</strong><cite>A Few Useful Things to Know about Machine Learning</cite></footer></blockquote></li><li><p>Another disadvantage of the algorithm is it only uses data points around the predicting points to make a decision and rest other data point are useless.</p></li></ol><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>We saw how we implemented the algorithm in python and also looked on how we could get maximum performance by choosing optimal value of K. We haven’t looked into everything related to KNN as there are family of algorithms that are based on KNN which fix the drawback we just discussed like weighted KNN which instead of just taking values on 0/1 from nearest neighbour it takes weighted average of the data points.</p></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Classification/">Classification</a><a class="link-muted mr-2" rel="tag" href="/tags/Regression/">Regression</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2017/10/02/visual-text-analytics-with-python/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Visual text Analytics with python</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2017/08/20/when-a-chatbot-meets-arduino/"><span class="level-item">when a chatbot meets arduino</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://www.taintedbits.com/2017/09/25/implementing-knn-from-scratch-in-python/",this.page.identifier="2017/09/25/implementing-knn-from-scratch-in-python/"};!function(){var t=document,e=t.createElement("script");e.src="//taintedbits.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#What-is-K-Nearest-Neighbour-Algorithm-KNN"><span class="level-left"><span class="level-item">1</span><span class="level-item">What is K-Nearest Neighbour Algorithm (KNN)?</span></span></a></li><li><a class="level is-mobile" href="#Calculating-distance-between-data-points"><span class="level-left"><span class="level-item">2</span><span class="level-item">Calculating distance between data points</span></span></a></li><li><a class="level is-mobile" href="#Implementing-KNN-classifier"><span class="level-left"><span class="level-item">3</span><span class="level-item">Implementing KNN classifier</span></span></a></li><li><a class="level is-mobile" href="#Implementing-KNN-Regression"><span class="level-left"><span class="level-item">4</span><span class="level-item">Implementing KNN Regression</span></span></a></li><li><a class="level is-mobile" href="#Optimized-the-computation-KNN-vectorized-version"><span class="level-left"><span class="level-item">5</span><span class="level-item">Optimized the computation (KNN vectorized version)</span></span></a></li><li><a class="level is-mobile" href="#Evaluate-the-performance-of-the-KNN"><span class="level-left"><span class="level-item">6</span><span class="level-item">Evaluate the performance of the KNN</span></span></a></li><li><a class="level is-mobile" href="#Data-pre-processing"><span class="level-left"><span class="level-item">7</span><span class="level-item">Data pre-processing</span></span></a></li><li><a class="level is-mobile" href="#Choosing-optimal-K-for-KNN"><span class="level-left"><span class="level-item">8</span><span class="level-item">Choosing optimal K for KNN</span></span></a></li><li><a class="level is-mobile" href="#Bias-vs-Variance-trade-off"><span class="level-left"><span class="level-item">9</span><span class="level-item">Bias vs Variance trade-off</span></span></a></li><li><a class="level is-mobile" href="#Advantages"><span class="level-left"><span class="level-item">10</span><span class="level-item">Advantages</span></span></a></li><li><a class="level is-mobile" href="#Disadvantages"><span class="level-left"><span class="level-item">11</span><span class="level-item">Disadvantages</span></span></a></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">12</span><span class="level-item">Conclusion</span></span></a></li></ul></div></div><style>#toc .menu-list>li>a.is-active+.menu-list{display:block}#toc .menu-list>li>a+.menu-list{display:none}</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2021-02-14T18:30:00.000Z">2021-02-15</time></p><p class="title"><a href="/2021/02/15/arm-architecture-webinar/">ARM Architecture and Shellcode Webinars</a></p><p class="categories"><a href="/categories/Binary-Exploitation/">Binary Exploitation</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-06-06T18:30:00.000Z">2020-06-07</time></p><p class="title"><a href="/2020/06/07/binary-exploitation-pwnable-kr-level-6/">Binary Exploitation [pwnable.kr] - (Level 6) random</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-06-05T18:30:00.000Z">2020-06-06</time></p><p class="title"><a href="/2020/06/06/binary-exploitation-pwnable-kr-level-5/">Binary Exploitation [pwnable.kr] - (Level 5) passcode</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-05-16T18:30:00.000Z">2020-05-17</time></p><p class="title"><a href="/2020/05/17/binary-exploitation-pwnable-kr-level-4/">Binary Exploitation [pwnable.kr] - (Level 4) flag</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-05-15T18:30:00.000Z">2020-05-16</time></p><p class="title"><a href="/2020/05/16/binary-exploitation-pwnable-kr-level-3/">Binary Exploitation [pwnable.kr] - (Level 3) BOF</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Binary-Exploitation/"><span class="level-start"><span class="level-item">Binary Exploitation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Brain-Logs/"><span class="level-start"><span class="level-item">Brain Logs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CTF-Writeups/"><span class="level-start"><span class="level-item">CTF Writeups</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Exploitation/"><span class="level-start"><span class="level-item">Exploitation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/IoT/"><span class="level-start"><span class="level-item">IoT</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Malware-Analysis/"><span class="level-start"><span class="level-item">Malware Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Reverse-Engineering/"><span class="level-start"><span class="level-item">Reverse Engineering</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ACS712/"><span class="tag">ACS712</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Arduino/"><span class="tag">Arduino</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bare-Metal/"><span class="tag">Bare-Metal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CTF/"><span class="tag">CTF</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Chat-Bots/"><span class="tag">Chat Bots</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Wrangling/"><span class="tag">Data Wrangling</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev/"><span class="tag">Dev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropper/"><span class="tag">Dropper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESP8266/"><span class="tag">ESP8266</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Editor/"><span class="tag">Editor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emacs/"><span class="tag">Emacs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Exploitation/"><span class="tag">Exploitation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GandCrab/"><span class="tag">GandCrab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Grey-Energy/"><span class="tag">Grey Energy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kernel-Debugging/"><span class="tag">Kernel Debugging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-Malware/"><span class="tag">Linux Malware</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Malware-Analysis/"><span class="tag">Malware Analysis</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Engineering/"><span class="tag">Reverse Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Visualization/"><span class="tag">Visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows-Malware/"><span class="tag">Windows Malware</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows-Reversing/"><span class="tag">Windows Reversing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/glibc/"><span class="tag">glibc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/javascript-obfuscation/"><span class="tag">javascript obfuscation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pwnable/"><span class="tag">pwnable</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pwnable-kr/"><span class="tag">pwnable-kr</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/radare2/"><span class="tag">radare2</span><span class="tag">4</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo-header.png" alt="Tainted Bits" height="28"></a><p class="is-size-7"><span>&copy; 2021 D3xt3r</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by-sa/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Subscribe" href="https://mailchi.mp/d744c1f04904/taintedbits"><i class="fa fa-envelope"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/0xd3xt3r"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now</a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load",(function(){outdatedBrowser({bgColor:"#f25648",color:"#ffffff",lowerThan:"object-fit"})}))</script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"Type something...",untitled:"(Untitled)",posts:"Posts",pages:"Pages",categories:"Categories",tags:"Tags"})}))</script></body></html>