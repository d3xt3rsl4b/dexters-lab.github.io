<!doctype html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>Reshaping Dataframe using Pivot and Melt in Apache Spark and pandas - Tainted Bits</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Tainted Bits"><meta name="msapplication-TileImage" content="/images/logo-header.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Tainted Bits"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Data cleaning is one of the most important and tedious part of data science workflow often mentioned but least discussed topic. Reflecting on my daily workflow, task of reshaping DataFrame is the very"><meta property="og:type" content="article"><meta property="og:title" content="Reshaping Dataframe using Pivot and Melt in Apache Spark and pandas"><meta property="og:url" content="https://www.taintedbits.com/2018/03/25/reshaping-dataframe-using-pivot-and-melt-in-apache-spark-and-pandas/"><meta property="og:site_name" content="Tainted Bits"><meta property="og:description" content="Data cleaning is one of the most important and tedious part of data science workflow often mentioned but least discussed topic. Reflecting on my daily workflow, task of reshaping DataFrame is the very"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.taintedbits.com/images/apache-spark-thumbnail.png"><meta property="article:published_time" content="2018-03-25T06:55:36.000Z"><meta property="article:modified_time" content="2021-03-11T10:27:20.905Z"><meta property="article:author" content="D3xt3r"><meta property="article:tag" content="Python"><meta property="article:tag" content="Data Wrangling"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/images/apache-spark-thumbnail.png"><meta property="twitter:creator" content="@0xd3xt3r"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.taintedbits.com/2018/03/25/reshaping-dataframe-using-pivot-and-melt-in-apache-spark-and-pandas/"},"headline":"Tainted Bits","image":["https://www.taintedbits.com/images/apache-spark-thumbnail.png"],"datePublished":"2018-03-25T06:55:36.000Z","dateModified":"2021-03-11T10:27:20.905Z","author":{"@type":"Person","name":"D3xt3r"},"description":"Data cleaning is one of the most important and tedious part of data science workflow often mentioned but least discussed topic. Reflecting on my daily workflow, task of reshaping DataFrame is the very"}</script><link rel="alternate" href="/atom.xml" title="Tainted Bits" type="application/atom+xml"><link rel="icon" href="/images/logo-header.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/railscasts.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-128181439-1" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-128181439-1")</script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo-header.png" alt="Tainted Bits" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/publications">Publication</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/0xd3xt3r"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time datetime="2018-03-25T06:55:36.000Z" title="3/25/2018, 12:25:36 PM">2018-03-25</time></span><span class="level-item">Updated&nbsp;<time datetime="2021-03-11T10:27:20.905Z" title="3/11/2021, 3:57:20 PM">2021-03-11</time></span><span class="level-item">12 minutes read (About 1765 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Reshaping Dataframe using Pivot and Melt in Apache Spark and pandas</h1><div class="content"><p>Data cleaning is one of the most important and tedious part of data science workflow often mentioned but least discussed topic. Reflecting on my daily workflow, task of reshaping DataFrame is the very common operation I often do to get the data in desired format. Reshaping dataframe means transformation of the table structure, may be remove/adding of columns/rows or doing some aggregations on certains rows and produce a new column to summerize the aggregation result. In this post I won’t cover everything about reshaping, but I will discuss two most frequently used operations i.e. <strong>pivot</strong> and <strong>melt</strong>. The solutions I discuss are in spark to be more specific pyspark and I will give you brief solution for pandas but if you want detail explanation of pandas solution I would recommend you to read <a href="/2017/12/04/data-cleaning-in-python-using-pandas/" title="this post">this post</a>.</p><span id="more"></span><h2 id="Pivoting-operation-on-data"><a href="#Pivoting-operation-on-data" class="headerlink" title="Pivoting operation on data"></a>Pivoting operation on data</h2><p>Data is usually stored in stacked format or record format, which can sometimes have repeated values which means the data is not normalized. When you want to summarize the data in a tabular view, <strong>pivot</strong> can be a very use transformation. Let take an example of online business which sell music and books and sales data(over simplified) is shown table below.</p><p><strong>TABLE A</strong></p><table><thead><tr><th>product</th><th>category</th><th>quarter</th><th>profit</th></tr></thead><tbody><tr><td>memories</td><td>book</td><td>q1</td><td>10</td></tr><tr><td>dreams</td><td>book</td><td>q2</td><td>20</td></tr><tr><td>reflections</td><td>book</td><td>q3</td><td>30</td></tr><tr><td>how to build a house</td><td>book</td><td>q4</td><td>40</td></tr><tr><td>wonderful life</td><td>music</td><td>q1</td><td>10</td></tr><tr><td>million miles</td><td>music</td><td>q2</td><td>20</td></tr><tr><td>run away</td><td>music</td><td>q3</td><td>30</td></tr><tr><td>mind and body</td><td>music</td><td>q4</td><td>40</td></tr></tbody></table><p>Above table has four columns, product and category are pretty much self explanatory, columns quarter and profit columns describe the profit contributed by the each product for that quarter. Some observation that can be made about table is that it is aggregation for the each product profit for each quarter but difficult to grasp in first look. This format of the table can be seen as:</p><ol><li><em>stacked format</em> : individual observation for each product is stacked on each other.</li><li><em>record format</em> : each row is the record for a song or a book.</li><li><em>long format</em> : if there are million music tables will be long and the columns are not too much.</li></ol><p>If we want to summarize the quarterly profit for each category of product in tabular format then the table below would have been more appropriate.</p><p><strong>TABLE B</strong></p><table><thead><tr><th>category</th><th>q1</th><th>q2</th><th>q3</th><th>q4</th></tr></thead><tbody><tr><td>music</td><td>10</td><td>20</td><td>30</td><td>40</td></tr><tr><td>book</td><td>10</td><td>20</td><td>30</td><td>40</td></tr></tbody></table><p>The table above is much more intuitive compared to TABLE A. This is what pivot operation will help us to achieve. Pivot will take unique value of a specific column/columns and turn it into one or more columns with the unique values of that column as the name of the columns, in our example q1-q4 were the unique value of the column <em>quarter</em> so a new columns is created for each quarter has column, this newly added columns are called <strong>pivot columns</strong>. Spark provides <strong>pivot</strong> functions in <strong>DataFrame</strong> object to for pivot transformation. Pivot functions requires four parameters the on which as as follows:</p><ol><li><em>Pivot column</em> is the column who’s unique values will become pivot columns. In case of our example <em>category</em> column is the pivot.</li><li><em>Value column</em> is the column whos value will be aggregated and mapped to <em>index column</em>, <em>profit</em> columns is what we are aggregating for this example so its value column.</li><li><em>Index column</em> is the column which you want to use it as a index for the pivot columns. For this example <em>category</em> is the index column.</li><li><em>Aggregation function</em> in case if there are more the one row for a the column we are pivoting on. We have used sum function for this example, if there are more then one row for <em>books</em> category for <em>q4</em> then we will sum profit for <em>q4</em>, but can change the aggregating function depending on question you are trying to answer. One could use average function to find average cost of book sold in each quarter.</li></ol><h3 id="Pivot-in-Spark"><a href="#Pivot-in-Spark" class="headerlink" title="Pivot in Spark"></a>Pivot in Spark</h3><p>Lets try some code example, below is the pyspark implementation of pivot transformation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"></span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create RDD for products</span></span><br><span class="line">data = sc.parallelize([</span><br><span class="line">    [<span class="string">&#x27;memories&#x27;</span>,<span class="string">&#x27;book&#x27;</span>,<span class="string">&#x27;q1&#x27;</span>,<span class="number">10</span>],</span><br><span class="line">    [<span class="string">&#x27;dreams&#x27;</span>,<span class="string">&#x27;book&#x27;</span>,<span class="string">&#x27;q2&#x27;</span>,<span class="number">20</span>],</span><br><span class="line">    [<span class="string">&#x27;reflections&#x27;</span>,<span class="string">&#x27;book&#x27;</span>,<span class="string">&#x27;q3&#x27;</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="string">&#x27;how to build a house&#x27;</span>,<span class="string">&#x27;book&#x27;</span>,<span class="string">&#x27;q4&#x27;</span>,<span class="number">40</span>],</span><br><span class="line">    [<span class="string">&#x27;wonderful life&#x27;</span>,<span class="string">&#x27;music&#x27;</span>,<span class="string">&#x27;q1&#x27;</span>,<span class="number">10</span>],</span><br><span class="line">    [<span class="string">&#x27;million miles&#x27;</span>,<span class="string">&#x27;music&#x27;</span>,<span class="string">&#x27;q2&#x27;</span>,<span class="number">20</span>],</span><br><span class="line">    [<span class="string">&#x27;run away&#x27;</span>,<span class="string">&#x27;music&#x27;</span>,<span class="string">&#x27;q3&#x27;</span>,<span class="number">30</span>],</span><br><span class="line">    [<span class="string">&#x27;mind and body&#x27;</span>,<span class="string">&#x27;music&#x27;</span>,<span class="string">&#x27;q4&#x27;</span>,<span class="number">40</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert the RDD to DataFrame</span></span><br><span class="line">df_products = sqlContext.createDataFrame(data, [<span class="string">&#x27;product&#x27;</span>,<span class="string">&#x27;category&#x27;</span>,<span class="string">&#x27;quarter&#x27;</span>,<span class="string">&#x27;profit&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># index column : category</span></span><br><span class="line"><span class="comment"># value column : profit</span></span><br><span class="line"><span class="comment"># pivot column : quarter</span></span><br><span class="line"><span class="comment"># agg function : sum</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># apply pivot on DataFrame DataFrame</span></span><br><span class="line">df_products.groupBy(<span class="string">&#x27;category&#x27;</span>).pivot(<span class="string">&#x27;quarter&#x27;</span>).<span class="built_in">sum</span>(<span class="string">&#x27;profit&#x27;</span>).show()</span><br></pre></td></tr></table></figure><p>We first create a RDD of the product then, create a DataFrame from that RDD. In the next step pivot transformation is applied. As you might have noticed that you don’t exclusively pass parameters to <em>pivot</em> function, pivot function only take the name of the pivot column. The result of the above code snippet is the TABLE B.</p><h3 id="Pivot-in-Pandas"><a href="#Pivot-in-Pandas" class="headerlink" title="Pivot in Pandas"></a>Pivot in Pandas</h3><p>Pivot method is also available in pandas library which take the same four parameters we described above, difference is in pandas just one method call will be provided with all the four parameters. The code below first converts the Spark’s Dataframe to pandas DataFrame and then apply <em>pivot__table</em> function on the DataFrame, resulting DataFrame will look like <em>TABLE B</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert from spark DataFrame to pandas DataFrame</span></span><br><span class="line">df_products_pd = df_products.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply pivot on pandas DataFrame</span></span><br><span class="line">df_products_pd.pivot_table(index=<span class="string">&#x27;category&#x27;</span></span><br><span class="line">                          ,columns=<span class="string">&#x27;quarter&#x27;</span></span><br><span class="line">                          ,values=<span class="string">&#x27;profit&#x27;</span></span><br><span class="line">                          ,aggfunc=<span class="built_in">sum</span>)</span><br></pre></td></tr></table></figure><h3 id="Common-Mistakes-in-Pivot-operation"><a href="#Common-Mistakes-in-Pivot-operation" class="headerlink" title="Common Mistakes in Pivot operation"></a>Common Mistakes in Pivot operation</h3><p>One of the common mistake people make while using pivot transformation is that they try to apply it on column with numeric values, while pivot function is suppose to be used on column with categorical values. In case of our example, we applied in on <em>quarter</em> column which has all categorical values.</p><h2 id="Melt-operation-on-data"><a href="#Melt-operation-on-data" class="headerlink" title="Melt operation on data"></a>Melt operation on data</h2><p>Melt transformation is opposite of of pivot transformation. With this operation data is converted from wide(unstacked) format to stacked/long format. Pivot operation help you to give a quick overview(tabular view) of the data, which is good for human analysis but difficult to do complex operations like grouping, etc. While the tables in wide format are to pretty to summarize data but difficult for analysis that where melt operation help us. Lets take and example of pewforum.org Income data of various religious group in the US. Tabular view of the data is as follows</p><table><thead><tr><th>religion</th><th>&lt;$10k</th><th>$10-20k</th><th>$20-30k</th><th>$30-40k</th><th>$40-50k</th><th>$50-75k</th><th>$75-100k</th><th>$100-150k</th><th>&gt;150k</th></tr></thead><tbody><tr><td>Agnostic</td><td>27</td><td>34</td><td>60</td><td>81</td><td>76</td><td>137</td><td>122</td><td>109</td><td>84</td></tr><tr><td>Atheist</td><td>12</td><td>27</td><td>37</td><td>52</td><td>35</td><td>70</td><td>73</td><td>59</td><td>74</td></tr><tr><td>Buddhist</td><td>27</td><td>21</td><td>30</td><td>34</td><td>33</td><td>58</td><td>62</td><td>39</td><td>53</td></tr><tr><td>Catholic</td><td>418</td><td>617</td><td>732</td><td>670</td><td>638</td><td>1116</td><td>949</td><td>792</td><td>633</td></tr></tbody></table><p>This table gives a good summarization of income of a particular religious group, but it would be good to have table with three column religion income and number of individual for that income, this format can help us to do some complex analysis like grouping the table by income category and finding which religious tradition is missing or present in a particular income category.</p><h3 id="Melt-in-Spark"><a href="#Melt-in-Spark" class="headerlink" title="Melt in Spark"></a>Melt in Spark</h3><p>Melt operation API is not provided by spark, but its not that difficult to create this operation. Below is the code to that implementations the melt function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> array, col, explode, lit, struct</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">melt_df</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        df: DataFrame,</span></span></span><br><span class="line"><span class="function"><span class="params">        id_vars: Iterable[<span class="built_in">str</span>], value_vars: Iterable[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">        var_name: <span class="built_in">str</span>=<span class="string">&quot;variable&quot;</span>, value_name: <span class="built_in">str</span>=<span class="string">&quot;value&quot;</span></span>) -&gt; DataFrame:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert :class:`DataFrame` from wide to long format.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create array&lt;struct&lt;variable: str, value: ...&gt;&gt;</span></span><br><span class="line">    _vars_and_vals = array(*(</span><br><span class="line">        struct(lit(c).alias(var_name), col(c).alias(value_name))</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> value_vars))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add to the DataFrame and explode</span></span><br><span class="line">    _tmp = df.withColumn(<span class="string">&quot;_vars_and_vals&quot;</span>, explode(_vars_and_vals))</span><br><span class="line"></span><br><span class="line">    cols = id_vars + [</span><br><span class="line">            col(<span class="string">&quot;_vars_and_vals&quot;</span>)[x].alias(x) <span class="keyword">for</span> x <span class="keyword">in</span> [var_name, value_name]]</span><br><span class="line">    <span class="keyword">return</span> _tmp.select(*cols)</span><br></pre></td></tr></table></figure><p>lets try the apply <strong>melt_df</strong> function on the religions group income dataset. <strong>melt_df</strong> function takes five parameters:</p><ol><li><strong>df</strong> : The Dataframe on which the operation will be carried out.</li><li><strong>id_vars</strong> : array of columns which will be the index to which the values of the columns to which matched to. In out example <em>religion</em> is the only id_vars, as we want to map it to various income class.</li><li><strong>value_vars</strong>: while id_vars help use to find the index of the values, this is the actual values will be extracted from these columns.</li><li><strong>var_name</strong>: the name of the variable column in the resulting DataFrame.</li><li><strong>value_name</strong>: this is the name of the value variable in the resulting DataFrame.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"></span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create RDD for products</span></span><br><span class="line">data = sc.parallelize([</span><br><span class="line">    [<span class="string">&#x27;Agnostic&#x27;</span> ,<span class="number">27</span>,<span class="number">34</span>,<span class="number">60</span>,<span class="number">81</span>,<span class="number">76</span>,<span class="number">137</span>,<span class="number">122</span>,<span class="number">109</span>,<span class="number">84</span>],</span><br><span class="line">    [<span class="string">&#x27;Atheist&#x27;</span>,<span class="number">12</span>,<span class="number">27</span>,<span class="number">37</span>,<span class="number">52</span>,<span class="number">35</span>,<span class="number">70</span>,<span class="number">73</span>,<span class="number">59</span>,<span class="number">74</span>],</span><br><span class="line">    [<span class="string">&#x27;Buddhist&#x27;</span>,<span class="number">27</span>,<span class="number">21</span>,<span class="number">30</span>,<span class="number">34</span>,<span class="number">33</span>,<span class="number">58</span>,<span class="number">62</span>,<span class="number">39</span>,<span class="number">53</span>],</span><br><span class="line">    [<span class="string">&#x27;Catholic&#x27;</span>,<span class="number">418</span>,<span class="number">617</span>,<span class="number">732</span>,<span class="number">670</span>,<span class="number">638</span>,<span class="number">1116</span>,<span class="number">949</span>,<span class="number">792</span>,<span class="number">633</span>],</span><br><span class="line">   ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># column headers</span></span><br><span class="line">table_columns = [<span class="string">&quot;religion&quot;</span>,<span class="string">&quot;&lt;$10k&quot;</span>,<span class="string">&quot;$10-20k&quot;</span>,<span class="string">&quot;$20-30k&quot;</span>,<span class="string">&quot;$30-40k&quot;</span>,</span><br><span class="line">                  <span class="string">&quot;$40-50k&quot;</span>,<span class="string">&quot;$50-75k&quot;</span>,<span class="string">&quot;$75-100k&quot;</span>,<span class="string">&quot;$100-150k&quot;</span>,<span class="string">&quot;&gt;150k&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#create the DataFrame</span></span><br><span class="line">df_rel_trad = sqlContext.createDataFrame(data, table_columns)</span><br><span class="line"></span><br><span class="line">df_rel = melt_df(df_rel_trad, [<span class="string">&#x27;religion&#x27;</span>], table_columns[<span class="number">1</span>:<span class="number">10</span>], <span class="string">&#x27;income&#x27;</span>, <span class="string">&#x27;count&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df_rel.show()</span><br></pre></td></tr></table></figure><p>The above code will give the output as shown in below table</p><table><thead><tr><th>religion</th><th>income</th><th>count</th></tr></thead><tbody><tr><td>Agnostic</td><td>&lt;$10k</td><td>27</td></tr><tr><td>Agnostic</td><td>$10-20k</td><td>34</td></tr><tr><td>Agnostic</td><td>$75-100k</td><td>122</td></tr><tr><td>Agnostic</td><td>&gt;150k</td><td>84</td></tr><tr><td>Atheist</td><td>&lt;$10k</td><td>12</td></tr><tr><td>Atheist</td><td>$10-20k</td><td>27</td></tr><tr><td>Atheist</td><td>$100-150k</td><td>59</td></tr><tr><td>Atheist</td><td>&gt;150k</td><td>74</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>Buddhist</td><td>&lt;$10k</td><td>27</td></tr><tr><td>Buddhist</td><td>$10-20k</td><td>21</td></tr></tbody></table><h3 id="Melt-in-pandas"><a href="#Melt-in-pandas" class="headerlink" title="Melt in pandas"></a>Melt in pandas</h3><p>pandas provides melt operator which is the snippet as below the parameters are same as explained previously. In the below example we have create a pandas dataframe and the applied melt operation the results are the same for the previous example.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># religious_df is the dataframe which stores above table</span></span><br><span class="line">In [<span class="number">1</span>]: value_variables = [<span class="string">&#x27;Less than $30000&#x27;</span>,<span class="string">&#x27;$30000-$49999&#x27;</span>,<span class="string">&#x27;$50000-$99999&#x27;</span>,<span class="string">&#x27;$100000 or more&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: religious_df = religious_df.melt(id_vars=[<span class="string">&#x27;Religious tradition&#x27;</span>], value_vars=value_variables)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: religious_df[religious_df[<span class="string">&#x27;Religious tradition&#x27;</span>] == <span class="string">&#x27;Buddhist&#x27;</span>]</span><br><span class="line"></span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">     Religious tradition          variable  value</span><br><span class="line"> <span class="number">0</span>             Buddhist  Less than $<span class="number">30000</span>     <span class="number">36</span></span><br><span class="line"> <span class="number">12</span>            Buddhist     $<span class="number">30000</span>-$<span class="number">49999</span>     <span class="number">18</span></span><br><span class="line"> <span class="number">24</span>            Buddhist     $<span class="number">50000</span>-$<span class="number">99999</span>     <span class="number">32</span></span><br><span class="line"> <span class="number">36</span>            Buddhist   $<span class="number">100000</span> <span class="keyword">or</span> more     <span class="number">13</span></span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We just saw how to implement the pivot and melt transformation which reshapes the DataFrame. To summarize what we have learnt pivot operation can be helpful to quickly summerize the table in tabular format which is easy for the human analysis. While on contrast data is tabular format is not quite helpful for complex analysis, this is where this melt operation converts the table from wide format to long format. There is quite a bit to say about which format is suitable in what situation which is topic of <a href="/2017/12/04/data-cleaning-in-python-using-pandas/" title="this post">this post</a>, I have discussed the details of various heustics of tidying the data.</p><h2 id="Useful-Links"><a href="#Useful-Links" class="headerlink" title="Useful Links"></a>Useful Links</h2><ol><li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.pivot">Spark Pivot API docs</a></li><li><a target="_blank" rel="noopener" href="https://databricks.com/blog/2016/02/09/reshaping-data-with-pivot-in-apache-spark.html">Databricks blog on pivot</a></li><li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe">Stackoverflow anwser of pyspark melt operation</a></li></ol></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Python/">Python</a><a class="link-muted mr-2" rel="tag" href="/tags/Data-Wrangling/">Data Wrangling</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/04/12/lets-emacs-in-21st-century/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Lets Emacs in 21st Century</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2017/12/04/data-cleaning-in-python-using-pandas/"><span class="level-item">Data cleaning in python using pandas</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://www.taintedbits.com/2018/03/25/reshaping-dataframe-using-pivot-and-melt-in-apache-spark-and-pandas/",this.page.identifier="2018/03/25/reshaping-dataframe-using-pivot-and-melt-in-apache-spark-and-pandas/"};!function(){var a=document,t=a.createElement("script");t.src="//taintedbits.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(a.head||a.body).appendChild(t)}()</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Pivoting-operation-on-data"><span class="level-left"><span class="level-item">1</span><span class="level-item">Pivoting operation on data</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Pivot-in-Spark"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Pivot in Spark</span></span></a></li><li><a class="level is-mobile" href="#Pivot-in-Pandas"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Pivot in Pandas</span></span></a></li><li><a class="level is-mobile" href="#Common-Mistakes-in-Pivot-operation"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Common Mistakes in Pivot operation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Melt-operation-on-data"><span class="level-left"><span class="level-item">2</span><span class="level-item">Melt operation on data</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Melt-in-Spark"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Melt in Spark</span></span></a></li><li><a class="level is-mobile" href="#Melt-in-pandas"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Melt in pandas</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">3</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#Useful-Links"><span class="level-left"><span class="level-item">4</span><span class="level-item">Useful Links</span></span></a></li></ul></div></div><style>#toc .menu-list>li>a.is-active+.menu-list{display:block}#toc .menu-list>li>a+.menu-list{display:none}</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2021-02-14T18:30:00.000Z">2021-02-15</time></p><p class="title"><a href="/2021/02/15/arm-architecture-webinar/">ARM Architecture and Shellcode Webinars</a></p><p class="categories"><a href="/categories/Binary-Exploitation/">Binary Exploitation</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-07-17T18:30:00.000Z">2020-07-18</time></p><p class="title"><a href="/2020/07/18/binary-exploitation-pwnable-tw-tcache-tear/">Binary Exploitation [pwnable.tw] - Tcache Tear</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-07-11T18:30:00.000Z">2020-07-12</time></p><p class="title"><a href="/2020/07/12/binary-exploitation-pwnable-tw-caov/">Binary Exploitation [pwnable.tw] - CAOV</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-07-10T18:30:00.000Z">2020-07-11</time></p><p class="title"><a href="/2020/07/11/binary-exploitation-pwnable-tw-spirited-away/">Binary Exploitation [pwnable.tw] - Spirited Away</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-07-04T18:30:00.000Z">2020-07-05</time></p><p class="title"><a href="/2020/07/05/binary-exploitation-pwnable-tw-realloc/">Binary Exploitation [pwnable.tw] - Realloc</a></p><p class="categories"><a href="/categories/CTF-Writeups/">CTF Writeups</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Binary-Exploitation/"><span class="level-start"><span class="level-item">Binary Exploitation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Brain-Logs/"><span class="level-start"><span class="level-item">Brain Logs</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/CTF-Writeups/"><span class="level-start"><span class="level-item">CTF Writeups</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Exploitation/"><span class="level-start"><span class="level-item">Exploitation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/IoT/"><span class="level-start"><span class="level-item">IoT</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Malware-Analysis/"><span class="level-start"><span class="level-item">Malware Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Reverse-Engineering/"><span class="level-start"><span class="level-item">Reverse Engineering</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ACS712/"><span class="tag">ACS712</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Arduino/"><span class="tag">Arduino</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bare-Metal/"><span class="tag">Bare-Metal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CTF/"><span class="tag">CTF</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Chat-Bots/"><span class="tag">Chat Bots</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Wrangling/"><span class="tag">Data Wrangling</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev/"><span class="tag">Dev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropper/"><span class="tag">Dropper</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESP8266/"><span class="tag">ESP8266</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Editor/"><span class="tag">Editor</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emacs/"><span class="tag">Emacs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Exploitation/"><span class="tag">Exploitation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GandCrab/"><span class="tag">GandCrab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Grey-Energy/"><span class="tag">Grey Energy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Heap-Exploitation/"><span class="tag">Heap Exploitation</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kernel-Debugging/"><span class="tag">Kernel Debugging</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux-Malware/"><span class="tag">Linux Malware</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Malware-Analysis/"><span class="tag">Malware Analysis</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROP/"><span class="tag">ROP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reverse-Engineering/"><span class="tag">Reverse Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Visualization/"><span class="tag">Visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WarGames/"><span class="tag">WarGames</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows-Malware/"><span class="tag">Windows Malware</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows-Reversing/"><span class="tag">Windows Reversing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/glibc/"><span class="tag">glibc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/javascript-obfuscation/"><span class="tag">javascript obfuscation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pwnable/"><span class="tag">pwnable</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pwnable-kr/"><span class="tag">pwnable-kr</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pwnable-tw/"><span class="tag">pwnable-tw</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/radare2/"><span class="tag">radare2</span><span class="tag">4</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo-header.png" alt="Tainted Bits" height="28"></a><p class="is-size-7"><span>&copy; 2021 D3xt3r</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by-sa/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Subscribe" href="https://mailchi.mp/d744c1f04904/taintedbits"><i class="fa fa-envelope"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/0xd3xt3r"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load",()=>{"function"==typeof $.fn.lightGallery&&$(".article").lightGallery({selector:".gallery-item"}),"function"==typeof $.fn.justifiedGallery&&($(".justified-gallery > p > .gallery-item").length&&$(".justified-gallery > p > .gallery-item").unwrap(),$(".justified-gallery").justifiedGallery())})</script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now</a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load",(function(){outdatedBrowser({bgColor:"#f25648",color:"#ffffff",lowerThan:"object-fit"})}))</script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",(function(){loadInsight({contentUrl:"/content.json"},{hint:"Type something...",untitled:"(Untitled)",posts:"Posts",pages:"Pages",categories:"Categories",tags:"Tags"})}))</script></body></html>